# GestureOS üñêÔ∏è

Control your Windows PC with hand gestures ‚Äî built with MediaPipe, TensorFlow, and a browser-based UI. No cloud, no subscription, runs fully local.

---

## What it does

- Live webcam gesture detection using MediaPipe hand landmarks
- Bind any gesture to a Windows action ‚Äî volume, brightness, media keys, open apps, screenshots, custom shell commands
- **Volume and brightness adjust continuously by pinching fingers in/out** (no button press needed)
- Train new gestures directly from the browser ‚Äî record ‚Üí train ‚Üí use
- Delete a gesture and the model automatically retrains without it

---

## Project Structure

```
GestureOS/
‚îÇ
‚îú‚îÄ‚îÄ index.html            ‚Üê Browser UI (camera, gesture library, training)
‚îú‚îÄ‚îÄ server.py             ‚Üê WebSocket server (MediaPipe + inference + OS actions)
‚îú‚îÄ‚îÄ controls.py           ‚Üê All Windows OS control functions + pinch logic
‚îú‚îÄ‚îÄ train_model.py        ‚Üê Neural network training script
‚îÇ
‚îú‚îÄ‚îÄ data/                 ‚Üê Auto-created ‚Äî one CSV per gesture
‚îÇ   ‚îú‚îÄ‚îÄ open_palm.csv
‚îÇ   ‚îî‚îÄ‚îÄ closed_fist.csv
‚îÇ
‚îú‚îÄ‚îÄ gesture_model.keras   ‚Üê Auto-created after first training
‚îú‚îÄ‚îÄ scaler.pkl            ‚Üê Auto-created after first training
‚îú‚îÄ‚îÄ label_encoder.pkl     ‚Üê Auto-created after first training
‚îÇ
‚îú‚îÄ‚îÄ env/                  ‚Üê Your virtual environment (not committed)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

---

## Requirements

- **Windows 10 or 11**
- **Python 3.9, 3.10, or 3.11** ‚Äî TensorFlow does not support 3.12 yet
- A webcam
- Chrome or Edge (camera requires `localhost`, not `file://`)

---

## Setup

### 1 ‚Äî Clone the repo

```bash
git clone https://github.com/your-username/GestureOS.git
cd GestureOS
```

### 2 ‚Äî Create a virtual environment

```bash
python -m venv env
```

Activate it:

```bash
# CMD
env\Scripts\activate.bat

# PowerShell (if you get an execution policy error, run this first once)
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
env\Scripts\Activate.ps1
```

You should see `(env)` at the start of your terminal prompt.

### 3 ‚Äî Install dependencies

```bash
pip install -r requirements.txt
```

TensorFlow is large ‚Äî this may take a few minutes on first install.

### 4 ‚Äî Start the WebSocket server

```bash
python server.py
```

Expected output:
```
Server running at ws://127.0.0.1:8765
```

Keep this terminal open.

### 5 ‚Äî Serve the frontend

Open a **second terminal**, activate env, then:

```bash
python -m http.server 8080
```

Open **http://localhost:8080** in Chrome or Edge.

> ‚ö†Ô∏è Never open `index.html` directly via `file://` ‚Äî the browser blocks camera access on file:// URLs.

---

## Usage

### Training a gesture

1. Click **+ Add Gesture**
2. Enter a name ‚Äî use `lowercase_with_underscores` (e.g. `open_palm`, `closed_fist`)
3. Choose a control (Volume Up, Play/Pause, Open Webpage, etc.)
4. Click **Save Gesture** ‚Äî the modal stays open
5. Click **‚óè Start Recording**, hold your hand gesture steady, wait for progress bar to fill (default 150 frames)
6. Click **‚ö° Train Now**
7. Repeat for each gesture ‚Äî **minimum 2 gestures needed** before training works

### Pinch-controlled volume and brightness

When a gesture is bound to **Volume Up/Down** or **Brightness Up/Down**, holding that gesture activates continuous pinch mode:

| Hand movement | Effect |
|--------------|--------|
| Spread thumb + index apart | Increase value |
| Pinch thumb + index together | Decrease value |
| Hold still | No change (dead zone) |

The change is smooth and proportional to how wide you spread.

### Deleting a gesture

Click üóë next to any gesture. This:
- Removes it from the UI
- Deletes its training CSV from `data/`
- Automatically retrains the model ‚Äî the gesture will no longer be recognised

---

## How the ML pipeline works

```
Webcam frame (browser)
        ‚Üì  WebSocket (base64 JPEG)
server.py receives frame
        ‚Üì
MediaPipe Hands ‚Äî 21 landmarks (x, y, z)
        ‚Üì
Normalise: subtract wrist, scale by wrist‚ÜíMCP9 distance ‚Üí 63 values
Add orientation features (thumb + index direction)        ‚Üí 69 values
        ‚Üì
StandardScaler  (scaler.pkl)
        ‚Üì
Dense neural network  (gesture_model.keras)
  Input(69) ‚Üí Dense(256) ‚Üí BN ‚Üí Dropout ‚Üí Dense(128) ‚Üí BN ‚Üí Dropout ‚Üí Softmax(N)
        ‚Üì
Smoothing: majority vote over last 5 frames
        ‚Üì
confidence ‚â• 70%  ‚Üí  controls.py fires the OS action
```

---

## File descriptions

| File | Role |
|------|------|
| `server.py` | WebSocket server at `ws://127.0.0.1:8765`. Receives frames, runs MediaPipe, runs model, sends predictions. Handles recording, training, deletion. |
| `controls.py` | All Windows OS actions. `execute_control(type, param)` for one-shot, `update_pinch(landmarks, mode)` for continuous pinch. |
| `train_model.py` | Reads CSVs from `data/`, trains the network, saves model + scaler + label encoder. Called by `server.py` automatically. |
| `index.html` | Single-file frontend ‚Äî no build step needed. Manages gesture library in `localStorage`, streams frames over WebSocket. |

---

## Troubleshooting

| Problem | Fix |
|---------|-----|
| `ModuleNotFoundError: pycaw` | `pip install pycaw comtypes` |
| `ModuleNotFoundError: screen_brightness_control` | `pip install screen-brightness-control` |
| Brightness not working | Some laptop displays don't expose software brightness control. Try using `Custom Command` with `nircmd.exe` instead |
| Camera not starting | Use `http://localhost:8080`, never `file://` |
| "No Model ‚Äî Train first" | Record data for at least 2 gestures, click Train |
| Gesture names not matching | Name in UI must exactly match the CSV label ‚Äî check Model Status panel |
| Low accuracy | Record 200+ frames, good consistent lighting, make gestures visually very different from each other |
| PowerShell execution policy error | `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser` |

---

## Tips for good accuracy

- Record **150‚Äì200+ frames** per gesture
- Use **consistent, even lighting** ‚Äî avoid windows behind you
- While recording, make **small natural variations** (slight rotation, distance changes) ‚Äî don't hold completely rigid
- Make gestures **visually distinct** ‚Äî two similar-looking gestures will confuse the model
- If accuracy is low, delete and re-record the problem gesture with more frames
